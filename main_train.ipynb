{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde30e57-4615-4acc-b593-c0651930465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import DataProcessor\n",
    "from parameters import set_model_params\n",
    "from network import ModelTrainer\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from _datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51ce7c6a-b5a1-4045-a49c-9f262b9eab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = \"./fairytales.txt\"\n",
    "SAVE_MODEL_PATH = \"save_models\"\n",
    "EPOCHS = 100\n",
    "VOCAB = DataProcessor.create_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66e18ae-ca8c-4f86-992b-f266f276ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    print(\"begin file processing\")\n",
    "    processor = DataProcessor(DIR_PATH)\n",
    "    print(\"dataset created\")\n",
    "    return processor\n",
    "\n",
    "def generate_model(dataset, input_length):\n",
    "    model_params = set_model_params(embedding_dim=input_length, hidden_dim=50, vocab_size=len(VOCAB), epochs=EPOCHS)\n",
    "    print(\"training_model\")\n",
    "    trainer = ModelTrainer(model_params, dataset)\n",
    "    print(\"Training completed\")\n",
    "    return trainer.get_model()\n",
    "\n",
    "def save_model(model):\n",
    "    timestamp = datetime.now().strftime(\"_%y%m%d_%H%M%S\")\n",
    "    model_filename = os.path.join(SAVE_MODEL_PATH, \"model\" + timestamp + \"epoch100.ser\")\n",
    "    torch.save(model, model_filename)\n",
    "    print(\"Model saved at path: \" + model_filename)\n",
    "    \n",
    "def invert_vocab():\n",
    "    return dict({(v, k) for k, v in VOCAB.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32004402-3876-4e8f-9fef-c90208780ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text from model\n",
    "# extra seed > 0 , specifies if to insert random values in between sequence\n",
    "def gen_text(model, extra_seed=0):\n",
    "    inv_vocab = invert_vocab()\n",
    "    inp_len = model.word_embeddings.embedding_dim\n",
    "    print(\"Generating sample text from model\")\n",
    "    print(\"===== ===== =====\")\n",
    "    para_len = random.randint(7, 20)\n",
    "    for _ in range(0, para_len):\n",
    "        line = []\n",
    "        seed = random.randint(1, 26)\n",
    "        line.append(seed)\n",
    "        line += [0] * (inp_len - 1)\n",
    "        seed_ins = False\n",
    "        for i in range(0, inp_len):\n",
    "            if extra_seed > 0 and i % (inp_len // extra_seed) == 5:\n",
    "                seed_ins = True\n",
    "            if seed_ins and line[i] == VOCAB[\" \"]:\n",
    "                ind = random.randint(1, 26)\n",
    "                seed_ins = False\n",
    "            else:\n",
    "                next_token = model(torch.tensor(line))\n",
    "                ind = torch.argmax(next_token[i], 0)\n",
    "                ind = ind.item()\n",
    "                # print(ind)\n",
    "            if i < inp_len - 1:\n",
    "                line[i + 1] = ind\n",
    "            else:\n",
    "                line.append(ind)\n",
    "        print(*[inv_vocab[i] for i in line if i > 0], sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aae90b9-5349-444d-9e32-ae9610878ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin file processing\n",
      "dataset created\n",
      "process file:  ./fairytales.txt\n",
      "Sequences transformed.\n",
      "Seq length  71\n",
      "training_model\n",
      "epoch 0 , run loss 2.268\n",
      "epoch 1 , run loss 1.832\n",
      "epoch 2 , run loss 1.699\n",
      "epoch 3 , run loss 1.618\n",
      "epoch 4 , run loss 1.566\n",
      "epoch 5 , run loss 1.525\n",
      "epoch 6 , run loss 1.495\n",
      "epoch 7 , run loss 1.468\n",
      "epoch 8 , run loss 1.445\n",
      "epoch 9 , run loss 1.421\n",
      "epoch 10 , run loss 1.401\n",
      "epoch 11 , run loss 1.385\n",
      "epoch 12 , run loss 1.369\n",
      "epoch 13 , run loss 1.356\n",
      "epoch 14 , run loss 1.347\n",
      "epoch 15 , run loss 1.336\n",
      "epoch 16 , run loss 1.328\n",
      "epoch 17 , run loss 1.320\n",
      "epoch 18 , run loss 1.310\n",
      "epoch 19 , run loss 1.303\n",
      "epoch 20 , run loss 1.297\n",
      "epoch 21 , run loss 1.291\n",
      "epoch 22 , run loss 1.285\n",
      "epoch 23 , run loss 1.279\n",
      "epoch 24 , run loss 1.274\n",
      "epoch 25 , run loss 1.270\n",
      "epoch 26 , run loss 1.266\n",
      "epoch 27 , run loss 1.260\n",
      "epoch 28 , run loss 1.255\n",
      "epoch 29 , run loss 1.252\n",
      "epoch 30 , run loss 1.247\n",
      "epoch 31 , run loss 1.243\n",
      "epoch 32 , run loss 1.239\n",
      "epoch 33 , run loss 1.236\n",
      "epoch 34 , run loss 1.232\n",
      "epoch 35 , run loss 1.229\n",
      "epoch 36 , run loss 1.227\n",
      "epoch 37 , run loss 1.222\n",
      "epoch 38 , run loss 1.219\n",
      "epoch 39 , run loss 1.217\n",
      "epoch 40 , run loss 1.213\n",
      "epoch 41 , run loss 1.209\n",
      "epoch 42 , run loss 1.208\n",
      "epoch 43 , run loss 1.203\n",
      "epoch 44 , run loss 1.202\n",
      "epoch 45 , run loss 1.200\n",
      "epoch 46 , run loss 1.197\n",
      "epoch 47 , run loss 1.194\n",
      "epoch 48 , run loss 1.193\n",
      "epoch 49 , run loss 1.190\n",
      "epoch 50 , run loss 1.189\n",
      "epoch 51 , run loss 1.187\n",
      "epoch 52 , run loss 1.182\n",
      "epoch 53 , run loss 1.183\n",
      "epoch 54 , run loss 1.180\n",
      "epoch 55 , run loss 1.178\n",
      "epoch 56 , run loss 1.177\n",
      "epoch 57 , run loss 1.175\n",
      "epoch 58 , run loss 1.172\n",
      "epoch 59 , run loss 1.171\n",
      "epoch 60 , run loss 1.171\n",
      "epoch 61 , run loss 1.167\n",
      "epoch 62 , run loss 1.166\n",
      "epoch 63 , run loss 1.164\n",
      "epoch 64 , run loss 1.163\n",
      "epoch 65 , run loss 1.161\n",
      "epoch 66 , run loss 1.159\n",
      "epoch 67 , run loss 1.158\n",
      "epoch 68 , run loss 1.157\n",
      "epoch 69 , run loss 1.157\n",
      "epoch 70 , run loss 1.155\n",
      "epoch 71 , run loss 1.153\n",
      "epoch 72 , run loss 1.154\n",
      "epoch 73 , run loss 1.153\n",
      "epoch 74 , run loss 1.148\n",
      "epoch 75 , run loss 1.148\n",
      "epoch 76 , run loss 1.146\n",
      "epoch 77 , run loss 1.146\n",
      "epoch 78 , run loss 1.147\n",
      "epoch 79 , run loss 1.144\n",
      "epoch 80 , run loss 1.141\n",
      "epoch 81 , run loss 1.141\n",
      "epoch 82 , run loss 1.141\n",
      "epoch 83 , run loss 1.139\n",
      "epoch 84 , run loss 1.140\n",
      "epoch 85 , run loss 1.138\n",
      "epoch 86 , run loss 1.137\n",
      "epoch 87 , run loss 1.136\n",
      "epoch 88 , run loss 1.135\n",
      "epoch 89 , run loss 1.133\n",
      "epoch 90 , run loss 1.134\n",
      "epoch 91 , run loss 1.132\n",
      "epoch 92 , run loss 1.131\n",
      "epoch 93 , run loss 1.132\n",
      "epoch 94 , run loss 1.131\n",
      "epoch 95 , run loss 1.130\n",
      "epoch 96 , run loss 1.126\n",
      "epoch 97 , run loss 1.127\n",
      "epoch 98 , run loss 1.124\n",
      "epoch 99 , run loss 1.125\n",
      "Training completed\n",
      "Model saved at path: save_models/model_230301_055958epoch100.ser\n",
      "Generating sample text from model\n",
      "===== ===== =====\n",
      "ze to the man was no hard to the start and the king who had never the\n",
      "xell the horse and the young man and the king and the king was a\n",
      "and the young man was and the man and the king and the king and the king\n",
      "very have the king and the king and the young man said the king and the\n",
      "jour he was stopped to the stand and the king was no more the king and\n",
      "ver the king was no more the king was no more the king was a little man\n",
      "in the king was the man was and the king and the king was and the king a\n",
      "fool the king who had begin to the surrost to the king was no more to\n",
      "and the king was a fing and the king was and the king and the king was\n",
      "could be and the princess and the king was no more the young man was\n",
      "for the king was the king was no more the king was as the king was\n",
      "and the king was the king was the young man was and the king was the\n",
      "when the young man and the youth and the king and the king and the king \n",
      "remen the king was not the stand and the king and the king and the king \n",
      "years and the king was so the king and the king was so the king was been\n",
      "ut to the stand and the king was no men and the king was the king was\n",
      "in the king was the king and the king was the king was and the young\n",
      "xisudded the man was no more the king was done to the man and the king a\n"
     ]
    }
   ],
   "source": [
    "processor = process_data()\n",
    "dataset = processor.create_dataset()\n",
    "input_length = processor.get_inp_len()\n",
    "print(\"Seq length \", input_length)\n",
    "model = generate_model(dataset, input_length)\n",
    "save_model(model)\n",
    "gen_text(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad9ddeda-472b-443f-942b-de223ed3abc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample text from model\n",
      "===== ===== =====\n",
      "quite little the king was no given the young man and the youth and\n",
      "be the thard the old man so the king was no moment and the king was\n",
      "come to just the youth and he was so the king and the king was and the\n",
      "next moming your majesty and the king was began to the youth was\n",
      "prince pallaning the king was a man and the king was a fear and the king\n",
      "find the king was no have the king of the magician and the king was the\n",
      "down the zill on the stand and the king was some the king and the boy\n",
      "ut to was the youth and the king and the king was down to the king and\n",
      "gook and down to the sun and the king was a finger and the king was\n",
      "and the story and the king was the king was no the handsome and the\n",
      "of the king was no more the should the king was a tree and the\n",
      "man and got to the prince was some day the king and the princess he\n"
     ]
    }
   ],
   "source": [
    "#saved_model = torch.load(\"./save_models/model_230301_023547.ser\")\n",
    "gen_text(model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5219b-14c3-400e-9c38-c31b4635fbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
