{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9699d7a6-5900-4187-8399-e51b7d999359",
   "metadata": {},
   "source": [
    "# 1. DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e601cc05-25f1-46c9-bd0d-d444f61215ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import numpy as np\n",
    "import argparse\n",
    "from Dataset import FairytalesDataset\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from train import save_checkpoint, load_checkpoint, train\n",
    "from models import lstm_model\n",
    "from test import predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e16d3df-70a6-4875-8f86-36e299196ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 53,   9,  73,   6,  19, 149,  40]), tensor([  9,  73,   6,  19, 149,  40,  93]))\n",
      "Vocab: 3028\n"
     ]
    }
   ],
   "source": [
    "DIR_PATH = \"data/fairytales.txt\"\n",
    "START_TOKEN = \"<s>\"\n",
    "END_TOKEN = \"</s>\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--max-epochs', type=int, default=0)\n",
    "parser.add_argument('--batch-size', type=int, default=256)\n",
    "parser.add_argument('--sequence_length', type=int, default=7)\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "dataset = FairytalesDataset(DIR_PATH, START_TOKEN, END_TOKEN, args)\n",
    "print(dataset.__getitem__(3))\n",
    "print('Vocab:', len(dataset.unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0886e175-1dc8-4b60-862a-5ac1bcc04764",
   "metadata": {},
   "source": [
    "1 batch = 256 sequences of 7 words\n",
    "In total, to cover the entire text, we have 146 batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fe765a-df1b-4488-b3ab-c1756eff2814",
   "metadata": {},
   "source": [
    "# 2. MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503a9c99-d7ee-4a61-b77e-89b7456985f5",
   "metadata": {},
   "source": [
    "Standard lstm model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93a26023-f05b-4372-9193-2b272a9cdeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Loading checkpoint\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lstm_model(dataset)\n",
    "train(dataset, model, args, load_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48376a-0cdf-4798-82b5-25bb55fbdf35",
   "metadata": {},
   "source": [
    "# 3. TESTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b8e2ec4-5e2b-466a-bd53-382a6df45d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " \n",
      "<s> there was once a king son who told his father that he had gone too before directly he had dazzled to make his grain </s> <s> one next fortune so what i wish to enter sure </s> <s> swiftly i will tell him with your majesty and asked that he will let me your answer and she about how he was different but before two asked that she was likely on a best great prince is three one though something the prince </s> <s> i did not carry lucky or three young man i have over the world and this first has left </s>\n",
      " \n",
      "Number of sentences: 4\n",
      " \n",
      "Perplexity per sentence: [1.197037462534054, 1.8803108189588493, 1.92846939064602, 1.9034254316188541]\n"
     ]
    }
   ],
   "source": [
    "text,sent_perplexity = predict(dataset, model, text='there was once', next_words=100)\n",
    "\n",
    "print('Generated text:')\n",
    "print(' ')\n",
    "print(text)\n",
    "print(' ')\n",
    "print('Number of sentences:', len(sent_perplexity))\n",
    "print(' ')\n",
    "print('Perplexity per sentence:', sent_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60b12e-7504-4070-9783-6628d690d0bf",
   "metadata": {},
   "source": [
    "# 3. METRIC "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a243d-ec73-4128-af06-a841367ec5c0",
   "metadata": {},
   "source": [
    "## Perplexity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d2350-ebbc-4d1a-8be6-71c431e521f0",
   "metadata": {},
   "source": [
    "**Perplexity** refers to the power of a probability distribution to predict, or assign probabilities, to a sample. \n",
    "\n",
    "Lower the perplexity value, the better the model. \n",
    "\n",
    "A model’s worst-case perplexity is fixed by the language’s vocabulary size.\n",
    "- If the model is completely dumb(worst possible), perplexity = |v| i.e. size of the vocabulary. \n",
    "\n",
    "Perplexity is model dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0f01a7-37e3-4667-a7e2-3ac8a64c7cb8",
   "metadata": {},
   "source": [
    "what we want is to compute the perplexity for each word \n",
    "Then compute compute the perplexity for each sentence of the text \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f8a57-ade5-4a76-95f8-5869f2d8c0c7",
   "metadata": {},
   "source": [
    "Cross-Entropy: how do we use the minimum amount of the memory to store a sequence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7c58d5-6f0e-43d5-8161-d2dc7483f9f6",
   "metadata": {},
   "source": [
    "Logits is the vector of raw (non-normalized) predictions that the model generates, which is ordinarily then passed to a normalization function. In this case, logits becomes the input to the softmax function. The softmax function then generates a vector of (normalized) probabilities with one value for each possible word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc04e88a-f37b-42da-9dad-4e2c9973dd39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
